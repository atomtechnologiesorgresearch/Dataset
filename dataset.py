# -*- coding: utf-8 -*-
"""Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v8QgthAZ9TyWhYcgOep8-6Vcbj8rZvt7
"""

from google.colab import drive
drive.mount('/content/drive')

import os
print("Current directory:", os.getcwd())
print("Drive contents:", os.listdir('/content/drive/MyDrive/'))

# dataset_dir = '/content/drive/MyDrive/openwebtext_dataset'
# os.makedirs(dataset_dir, exist_ok=True)
# print(f"Created directory: {dataset_dir}")

!pip install -q datasets huggingface_hub tqdm zstandard

# from datasets import load_dataset
# import gzip, json
# from tqdm.auto import tqdm

# # Load the OpenWebText mirror (compatible with latest datasets)
# ds = load_dataset("vietgpt/openwebtext_en", split="train", streaming=True)

# out_path = "/content/drive/MyDrive/Dataset/openwebtext.jsonl.gz"   # change if needed
# cnt = 0

# with gzip.open(out_path, "wt", encoding="utf-8") as fout:
#     for doc in tqdm(ds, desc="Writing docs"):
#         fout.write(json.dumps(doc, ensure_ascii=False) + "\n")
#         cnt += 1
#         # optional: stop early for testing
#         # if cnt >= 100000: break

# print(f"✅ Done — written {cnt} documents to {out_path}")

import gzip, json

cnt = 0
with gzip.open("/content/drive/MyDrive/Dataset/openwebtext.jsonl.gz", "rt", encoding="utf-8") as fin:
    for _ in fin:
        cnt += 1
print("Total documents:", cnt)

!pip install --upgrade pip
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
!pip install transformers accelerate bitsandbytes

!nvidia-smi

!pip install -q torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121
!pip install -q transformers datasets accelerate bitsandbytes sentencepiece

import torch
print("CUDA available:", torch.cuda.is_available())
print("GPU name:", torch.cuda.get_device_name(0))
print("Memory allocated:", round(torch.cuda.memory_allocated(0)/1024**3, 2), "GB")

!pip install -q transformers accelerate bitsandbytes sentencepiece

from transformers import AutoModelForCausalLM, AutoTokenizer

MODEL_NAME = "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T"

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",          # puts layers on GPU
    load_in_4bit=True,          # quantize to 4-bit
    torch_dtype="auto",
)

print("Loaded:", MODEL_NAME)

inputs = tokenizer("Hello, how are you?", return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# # ================== DRIVE MOUNT ==================
# from google.colab import drive
# drive.mount('/content/drive', force_remount=True)

# # ================== CONFIG ==================
# OUT_DIR  = "/content/drive/MyDrive/outputDatadir"
# INPUT_PATH = "/content/drive/MyDrive/Dataset/openwebtext.jsonl.gz"
# MODEL_NAME = "microsoft/phi-1_5"

# N_LINES = 15          # always take 15 lines
# GEN_TOKENS = 256
# MAX_LEN = 2048        # phi-1_5 context window
# # =============================================

# import os, re, gzip, json
# import torch
# from transformers import AutoTokenizer, AutoModelForCausalLM

# os.makedirs(OUT_DIR, exist_ok=True)

# # ---------- Load model ----------
# print("Loading model:", MODEL_NAME)
# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
# model = AutoModelForCausalLM.from_pretrained(
#     MODEL_NAME,
#     device_map="auto",
#     load_in_4bit=True,
#     torch_dtype="auto"
# )
# model.eval()
# device = next(model.parameters()).device
# MODEL_MAX_LEN = tokenizer.model_max_length or MAX_LEN
# print(f"Loaded {MODEL_NAME} on {device} (max_len={MODEL_MAX_LEN})")

# # ---------- Input helper ----------
# def open_input(path):
#     if path.endswith(".gz"):
#         return gzip.open(path, "rt", encoding="utf-8", errors="ignore")
#     return open(path, "r", encoding="utf-8", errors="ignore")

# def extract_text(line):
#     try:
#         obj = json.loads(line)
#         if isinstance(obj, dict) and "text" in obj:
#             return obj["text"]
#         if isinstance(obj, str):
#             return obj
#         if isinstance(obj, dict):
#             for v in obj.values():
#                 if isinstance(v, str) and len(v) > 20:
#                     return v
#     except Exception:
#         return None
#     return None

# # ---------- Build prompt ----------
# def build_prompt(lines):
#     instr = (
#         "You are a text rewriter. For each input line, output in this exact format:\n"
#         "Q: <original line>\n"
#         "A: <rewritten line>\n\n"
#         f"Do this for exactly {len(lines)} lines. No extra commentary.\n\n"
#         "=== INPUT LINES ===\n"
#     )
#     return instr + "\n".join(lines) + "\n\n=== OUTPUT ===\n"

# # ---------- Run LLM ----------
# def run_llm(prompt):
#     try:
#         inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=MODEL_MAX_LEN).to(device)
#         with torch.no_grad():
#             outputs = model.generate(
#                 **inputs,
#                 max_new_tokens=GEN_TOKENS,
#                 do_sample=True,
#                 temperature=0.7,
#                 top_p=0.9,
#                 eos_token_id=tokenizer.eos_token_id,
#                 pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,
#             )
#         out_ids = outputs[0][inputs["input_ids"].shape[1]:]
#         text = tokenizer.decode(out_ids, skip_special_tokens=True).strip()
#         return text
#     except Exception as e:
#         return f"ERROR: {e}"

# # ---------- Main test ----------
# lines = []
# with open_input(INPUT_PATH) as fin:
#     for _ in range(N_LINES * 5):   # read extra to skip empties
#         raw = fin.readline()
#         if raw == "":
#             break
#         txt = extract_text(raw)
#         if txt:
#             lines.append(txt.strip())
#         if len(lines) >= N_LINES:
#             break

# if not lines:
#     raise ValueError("No valid lines found in dataset!")

# print(f"Collected {len(lines)} lines, sending to model...")

# prompt = build_prompt(lines)
# output = run_llm(prompt)

# out_file = os.path.join(OUT_DIR, "qa_batch_test.txt")
# with open(out_file, "w", encoding="utf-8") as f:
#     f.write(output)

# print("\nSaved Q/A batch to:", out_file)
# print("\n--- Preview ---\n")
# print(output[:800])

# ================== DRIVE MOUNT ==================
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# ================== CONFIG ==================
OUT_DIR  = "/content/drive/MyDrive/data"
INPUT_PATH = "/content/drive/MyDrive/Dataset/openwebtext.jsonl.gz"
MODEL_NAME = "microsoft/phi-1_5"

N_LINES = 15          # always 15 lines per batch
GEN_TOKENS = 256
MAX_LEN = 2048
# =============================================

import os, re, gzip, json, time
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

os.makedirs(OUT_DIR, exist_ok=True)

# ---------- Load model ----------
print("Loading model:", MODEL_NAME)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",
    load_in_4bit=True,
    torch_dtype="auto"
)
model.eval()
device = next(model.parameters()).device
MODEL_MAX_LEN = tokenizer.model_max_length or MAX_LEN
print(f"Loaded {MODEL_NAME} on {device} (max_len={MODEL_MAX_LEN})")

# ---------- Helpers ----------
def open_input(path):
    if path.endswith(".gz"):
        return gzip.open(path, "rt", encoding="utf-8", errors="ignore")
    return open(path, "r", encoding="utf-8", errors="ignore")

def extract_text(line):
    try:
        obj = json.loads(line)
        if isinstance(obj, dict) and "text" in obj:
            return obj["text"]
        if isinstance(obj, str):
            return obj
        if isinstance(obj, dict):
            for v in obj.values():
                if isinstance(v, str) and len(v) > 20:
                    return v
    except Exception:
        return None
    return None

def build_prompt(lines):
    instr = (
        "You are a text rewriter. For each input line, output in this exact format:\n"
        "Q: <original line>\n"
        "A: <rewritten line>\n\n"
        f"Do this for exactly {len(lines)} lines. No extra commentary.\n\n"
        "=== INPUT LINES ===\n"
    )
    return instr + "\n".join(lines) + "\n\n=== OUTPUT ===\n"

def run_llm(prompt):
    try:
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=MODEL_MAX_LEN).to(device)
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=GEN_TOKENS,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,
            )
        out_ids = outputs[0][inputs["input_ids"].shape[1]:]
        text = tokenizer.decode(out_ids, skip_special_tokens=True).strip()
        return text
    except Exception as e:
        return f"ERROR: {e}"

# ---------- Main loop ----------
def main():
    batch_idx = 1
    buf = []

    with open_input(INPUT_PATH) as fin:
        for line in fin:
            txt = extract_text(line)
            if not txt:
                continue
            txt = txt.strip()
            if not txt:
                continue

            buf.append(txt)
            if len(buf) >= N_LINES:
                # build prompt + run model
                prompt = build_prompt(buf)
                print(f"\n>>> Processing batch {batch_idx} (lines={len(buf)})...")
                output = run_llm(prompt)

                # save output
                out_file = os.path.join(OUT_DIR, f"qa_batch_{batch_idx:05d}.txt")
                with open(out_file, "w", encoding="utf-8") as f:
                    f.write(output)
                print("✓ Saved:", out_file)

                # reset for next batch
                buf = []
                batch_idx += 1
                time.sleep(0.5)

        # process last partial batch
        if buf:
            prompt = build_prompt(buf)
            print(f"\n>>> Processing final batch {batch_idx} (lines={len(buf)})...")
            output = run_llm(prompt)
            out_file = os.path.join(OUT_DIR, f"qa_batch_{batch_idx:05d}.txt")
            with open(out_file, "w", encoding="utf-8") as f:
                f.write(output)
            print("✓ Saved:", out_file)

    print("\nAll done. Total batches:", batch_idx)

# --------- Run ---------
main()

import os

# Path to your output directory
OUT_DIR = "/content/drive/MyDrive/data"

total_chars = 0
file_count = 0

for fname in os.listdir(OUT_DIR):
    fpath = os.path.join(OUT_DIR, fname)
    if os.path.isfile(fpath) and fname.endswith(".txt"):
        with open(fpath, "r", encoding="utf-8", errors="ignore") as f:
            total_chars += len(f.read())
        file_count += 1

print(f"Scanned {file_count} files.")
print(f"Total characters across all files: {total_chars:,}")

# # because of less compute the outfiles are 914 which is extremely small for training a tokenizer
# # ================== MOUNT DRIVE ==================

# from google.colab import drive
# drive.mount('/content/drive')

# # ================== INSTALL & IMPORT ==================
# !pip install datasets -q

# from datasets import load_dataset
# import json, os

# # ================== CONFIG ==================
# OUT_PATH = "/content/drive/MyDrive/data/arxiv_cs_subset.jsonl"  # where to save
# DATASET = "CShorten/ML-ArXiv-Papers"  # or "ashish-chouhan/arxiv_cs_papers"

# # ================== DOWNLOAD DATASET ==================
# print(f"Downloading {DATASET}...")
# ds = load_dataset(DATASET)

# print("Dataset loaded. Example row:")
# print(ds["train"][0])

# # ================== SAVE TO DRIVE ==================
# with open(OUT_PATH, "w", encoding="utf-8") as fout:
#     for row in ds["train"]:
#         fout.write(json.dumps(row, ensure_ascii=False) + "\n")

# print(f"\n✓ Saved CS subset to: {OUT_PATH}")
# print(f"Size on disk: {os.path.getsize(OUT_PATH)/1024/1024:.2f} MB")

# # ================== MOUNT DRIVE ==================
# from google.colab import drive
# drive.mount('/content/drive')

# # ================== INSTALL KAGGLE API ==================
# !pip install kaggle -q

# import os

# # ================== CONFIG ==================
# OUT_DIR = "/content/drive/MyDrive/arxiv_dataset"
# os.makedirs(OUT_DIR, exist_ok=True)

# # Make sure you have your Kaggle API key saved as kaggle.json in your Drive
# KAGGLE_JSON = "/content/drive/MyDrive/kaggle.json"

# # ================== SETUP KAGGLE CREDENTIALS ==================
# !mkdir -p ~/.kaggle
# !cp {KAGGLE_JSON} ~/.kaggle/
# !chmod 600 ~/.kaggle/kaggle.json

# # ================== DOWNLOAD FULL ARXIV SNAPSHOT ==================
# print("Downloading full arXiv dataset (~2.5 GB)...")
# !kaggle datasets download -d Cornell-University/arxiv -p {OUT_DIR}

# # ================== UNZIP ==================
# print("Unzipping dataset...")
# !unzip -q {OUT_DIR}/arxiv.zip -d {OUT_DIR}

# print("\n✓ Done! Dataset saved to:", OUT_DIR)
# !ls -lh {OUT_DIR}